<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Venkatesh Mullur - Index</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/professional-portfolio.png" rel="professional-portfolio">
  <link href="./assets/img/professional-portfolio.png" rel="professional-portfolio">
  <link rel="icon" href="./assets/img/professional-portfolio.png">
  <!-- <link rel="icon" href="./assets/img/portfolio.png"> -->

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="./assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="./assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="./assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="./assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="./assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: iPortfolio - v3.10.0
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>
<style>
  h5 {text-align: center;}
</style>

<body>


    <!-- ======= Resume Section =======  ../index.html -->
    <section id="resume" class="resume">
      <div class="container">         

        <div class="section-title d-flex">
            <a href="../index.html#portfolio"> 
                <img src="./assets/img/back.png" alt="Back" style="height: 20px; width: 20px; margin-top: 15px; margin-right: 10px;">
              </a>
              
          <h2>Tailoring Room Conditions for Optimal Stress Reduction using Machine Learning</h2>

          <a href="https://github.com/venk221/human_pose_estimation-using-Zed2i-camera">
            <img src="./assets/img/github.png" alt="Back" style="height: 40px; width: 40px; margin-top: 0px; margin-right: 10px; margin-left: 25px;">
          </a>
        </div>

        <h3>Abstract</h3>
        <p>
            This project investigates the correlation between self-reported emotional states and observed behaviors and physiological signals in personalized environment control rooms. Participants undergo emotional assessments using PANAS questionnaires before and after experiencing various room conditions. During their time in the room, activities are recorded via video and EEG signals, while facial emotion detection and human pose estimation techniques are employed to analyze behavior. The aim is to identify optimal room configurations that induce relaxation and reduce stress levels based on individual emotional responses. Results demonstrate the potential for tailored interventions to enhance stress reduction experiences in controlled environments.
        </p>
          
            <div class="row">
                    <div class="col-lg-4" data-aos="fade-right">
                      <img src="./assets/img/Tailoring Room Conditions for Optimal Stress Reduction using Machine Learning/room.jpeg" class="img-fluid" alt="" style="margin-top: 200px;">
                      <p class="image-caption">Personalized Environmental Control Room</p>
                      <img src="./assets/img/Tailoring Room Conditions for Optimal Stress Reduction using Machine Learning/output_video.gif" class="img-fluid" alt="" style="margin-top: 100px;">
                      <p class="image-caption">Previously recorded Human Pose Estimation</p>
                      <img src="./assets/img/Tailoring Room Conditions for Optimal Stress Reduction using Machine Learning/PANAS_gui.png" class="img-fluid" alt="" style="margin-top: 150px;">
                      <p class="image-caption">GUI for integration of all sensors</p>
                      <img src="./assets/img/Tailoring Room Conditions for Optimal Stress Reduction using Machine Learning/output_mve.png" class="img-fluid" alt="" style="margin-top: 300px;">
                      <p class="image-caption">Human Pose Estimation using Zed 2i Camera</p>
                    </div>
                    <div class="col-lg-8 pt-4 pt-lg-0 content" data-aos="fade-left">
                      <h3>Introduction</h3>
                        <p class="paragraph-spacing">
                              This project explores the correlation between changes in environmental factors and human emotions by analyzing data from a personalized environment control room. Utilizing a questionnaire (Positive and Negative Affect Schedule - PANAS) and audio recordings, participants' emotional states are assessed before and after their experiences in the room. Various sensors, including EEG and video capture, monitor participants' responses, while interactive activities aim to reduce stress levels. Through facial emotion detection and human pose estimation, the project aims to map these physiological responses to emotional changes indicated by PANAS scores. The ultimate objective is to identify room conditions that induce relaxation and reduce stress for individuals based on their initial emotional states.
                        </p>
                        <p class="paragraph-spacing">
                              Understanding the impact of environmental stimuli on human emotions is crucial for designing spaces conducive to well-being and productivity. This project investigates the complex relationship between environmental factors and emotional responses through a controlled experiment conducted in a personalized environment control room. By integrating multi-modal data collection techniques and advanced analytics, the study aims to uncover patterns linking environmental adjustments to changes in emotional states.
                        </p>

                    <h3 style="margin-bottom: 40px; margin-top: 30px;">Problem Statement</h3>
                    <p class="paragraph-spacing">
                        Despite advancements in sensor technology, accurately quantifying human emotions in real-time remains challenging. Traditional machine learning approaches are hindered by the lack of labeled data for emotion recognition, necessitating innovative methods that can capture emotional dynamics without relying on pre-defined categories. Additionally, noise and occlusions in sensor data, such as those from facial and pose detection systems, further complicate emotion inference. This project seeks to address these challenges by developing unsupervised learning algorithms capable of discerning emotional patterns from multi-modal sensor data, thereby facilitating the identification of environmental conditions conducive to relaxation.
                    </p>

                    <h3 style="margin-bottom: 30px; margin-top: 30px;">Methodology</h3>
                    <p class="paragraph-spacing">
                        The project employs a combination of data collection methods, including PANAS questionnaires, audio recordings, video capture, EEG sensors, and environmental control interfaces. Facial emotion detection and human pose estimation algorithms analyze video data, while unsupervised learning techniques, such as CNN and LSTM autoencoders, process keypoint information extracted from pose data. Gaussian Mixture Models (GMM) and Kalman filtering are utilized to mitigate noise and enhance the accuracy of keypoint detection. Augmentation techniques are applied to address data scarcity and preserve spatial constraints inherent in keypoint representations. Integration challenges, such as interfacing the Zed 2i camera with the graphical user interface (GUI), are also addressed to ensure seamless data acquisition.
                        <ul1>
                              <h5>Data Collection Methods:</h5>
                              The project employs a comprehensive approach to data collection, utilizing various instruments to capture different aspects of participant experiences. The PANAS questionnaire provides self-reported emotional states, while audio recordings offer additional insight into vocal cues and tonal variations. Video capture serves to visually record participants' behaviors and expressions, while EEG sensors monitor physiological responses associated with emotional arousal. Environmental control interfaces allow researchers to manipulate factors such as lighting, music, and spatial layout, enabling precise control over experimental conditions.
                              <h5>Facial Emotion Detection and Human Pose Estimation:</h5>
                              Advanced algorithms for facial emotion detection and human pose estimation play a crucial role in analyzing video data. These algorithms automatically identify facial expressions and body postures, providing quantitative measures of emotional responses and physical activities. By extracting keypoint information from video frames, researchers can track participants' movements and facial expressions over time, facilitating the analysis of emotional dynamics during their interaction with the environment.
                              <h5>Noise Mitigation and Data Augmentation:</h5>
                              To address noise and enhance the accuracy of keypoint detection, Gaussian Mixture Models (GMM) and Kalman filtering techniques are applied. GMM helps identify outliers and suppress noise in keypoint data, while Kalman filtering provides a recursive estimation of keypoint trajectories, improving tracking accuracy. Augmentation techniques are utilized to expand the dataset and preserve spatial constraints inherent in keypoint representations. These augmentation strategies ensure robust model training and generalization, despite limited data availability.
                              <h5>Integration Challenges:</h5>
                              Integration efforts are focused on overcoming technical hurdles, such as interfacing the Zed 2i camera with the graphical user interface (GUI). Seamless integration of hardware and software components ensures smooth data acquisition and real-time monitoring of participant interactions. By addressing integration challenges, researchers can streamline the experimental process and facilitate comprehensive data analysis, ultimately advancing our understanding of the relationship between environmental stimuli and human emotions.
                        </ul1>
                  </p>
                  In summary, the project employs a multidisciplinary approach, integrating diverse data collection methods, advanced algorithms, and integration solutions to investigate the intricate interplay between environmental factors and human emotions. Through rigorous experimentation and data analysis, researchers aim to uncover actionable insights for designing environments that promote emotional well-being and stress reduction.
                    


                  <h3 style="margin-bottom: 40px; margin-top: 30px;">Results</h3>
                  <p class="paragraph-spacing">
                        The preliminary findings of the project present compelling evidence of correlations between environmental adjustments and changes in participant emotions. Through meticulous analysis of multi-modal sensor data, including inputs from PANAS questionnaires, audio recordings, video capture, EEG sensors, and environmental control interfaces, researchers have begun to uncover intricate relationships between environmental stimuli and emotional responses.
                  </p>
                  <p class="paragraph-spacing">
                        One key takeaway from the initial analysis is the potential of developed unsupervised learning algorithms in capturing nuanced emotional dynamics. These algorithms, trained to discern patterns within the diverse streams of sensor data, exhibit promising capabilities in identifying subtle shifts in emotional states. Despite the presence of noise and occlusions inherent in real-world data, the algorithms demonstrate robustness in detecting and interpreting emotional cues, contributing to the reliability of emotion inference.
                  </p>
                  <p class="paragraph-spacing">
                        To further enhance the effectiveness of emotion inference models, ongoing efforts focus on refining data processing techniques. Augmentation strategies are being explored to expand the dataset and improve model generalization. By synthetically generating additional data points, researchers aim to address data scarcity issues and enhance the robustness of trained models. Additionally, sensor fusion approaches are being investigated to integrate information from multiple modalities, such as facial expressions, body movements, and physiological signals, into a cohesive framework for emotion analysis.
                  </p>
                  <p class="paragraph-spacing">
                        Integration efforts are also underway to streamline data collection and analysis processes. Challenges related to interfacing hardware components, such as the Zed 2i camera, with the graphical user interface (GUI) are being addressed to ensure seamless data acquisition. By optimizing data collection workflows and enhancing system interoperability, researchers aim to facilitate comprehensive insights into the complex interplay between environmental factors and human emotions.
                  </p>
                  <p class="paragraph-spacing">
                        Overall, the preliminary findings underscore the potential of leveraging multi-modal sensor data and unsupervised learning techniques to gain deeper insights into the relationship between environmental stimuli and human emotions. As the project progresses, continued refinement of data processing methods and integration efforts will contribute to the development of robust emotion inference models, ultimately informing the design of personalized environments conducive to emotional well-being and stress reduction.
                  </p>
                  </div>
            </div>
          
                </div>
              </section>
          <!-- <h2>Structure from Motion</h2>
          
            <h3>The GitHub link is</h3>
            <div class="portfolio-links">
                <a href="https://github.com/venk221/Structure-from-Motion-NeRF" title="More Details"><i class="bx bx-link"></i></a>
            </div> -->


        </div>

    <!-- End Resume Section -->

    <!-- ======= Portfolio Section ======= -->
    <section id="portfolio" class="portfolio section-bg">
      <div class="container">
    
        <div class="section-title">
          <h2>Portfolio</h2>
          <p>Find my projects on my github profile.</p>
        </div>
    
        <div class="portfolio-grid">
    
            <div class="portfolio-item">
              <a href="../test.html">
                <div class="portfolio-wrap">
                  <img src="./assets/img/portfolio/sfm.gif" class="img-fluid" alt="">
                  <h5> Neural  Radiance Fields </h5>
                </div>
              </a>
            </div>
    
            <div class="portfolio-item">
              <a href="../sfm.html">
                <div class="portfolio-wrap">
                  <img src="./assets/img/portfolio/portfolio-2.jpg" class="img-fluid" alt="">
                  <h5> Structure from Motion </h5>
                  <div class="portfolio-links">
                  </div>
                </div>
              </a>
            </div>
            <!-- portfolio/portfolio-3.jpg -->
            <div class="portfolio-item">
              <a href="../pc.html">
              <div class="portfolio-wrap">
                <img src="./assets/img/hero-bg.gif" class="img-fluid" alt="">
                <h5> Point cloud Semantic Segmentation </h5>
                <div class="portfolio-links">
                </div>
              </div>
            </a>
            </div>

          
            <div class="portfolio-item">
              <a href="../thesis_DR.html">
              <div class="portfolio-wrap">
                <img src="./assets/img/portfolio/o1.png" class="img-fluid" alt="" >
                <h5> Image Inpaining Based Keypoint Tracking</h5>
                <div class="portfolio-links">
                </div>
              </div>
            </a>
            </div>


            <!-- <div class="portfolio-item">
              <a href="../Machine_vision_engineer.html">
              <div class="portfolio-wrap">
                <img src="./assets/img/portfolio/output_video.gif" class="img-fluid" alt="" >
                <h5>Tailoring Room Conditions for Optimal Stress Reduction using Machine Learning </h5>
                <div class="portfolio-links">
                </div>
              </div>
            </a>
            </div> -->
          

          <div class="portfolio-item">
            <a href="../vio.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/VIO.gif" class="img-fluid" alt="">
              <h5> Visual Inertial Odometry using MSCKF </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../calib.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/calib.png" class="img-fluid" alt="">
              <h5> Automatic Camera Calibration </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>
          

          <div class="portfolio-item">
            <a href="../deepfakes.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/portfolio-5.jpg" class="img-fluid" alt="">
              <h5> DeepFakes</h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../bert.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/bert_finetune.jpeg" class="img-fluid" alt="">
              <h5> BERT and LoraBERT fine-tuning for Sentiment Analysis</h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>


          <div class="portfolio-item">
            <a href="../Panaroma.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/portfolio-8.jpg" class="img-fluid" alt="">
              <h5> Image Stitching using SIFT Features </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../lstm.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/lstm_img1.jpeg" class="img-fluid" alt="">
              <h5> Time-Series Analysis using LSTM </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../fake_news.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/nltk.png" class="img-fluid" alt="">
              <h5> Fake News Detection using NLP and Machine Learning Techniques </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>



          <div class="portfolio-item">
            <a href="../C.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/portfolio-7.jpg" class="img-fluid" alt="">
              <h5> Trajectory Tracking of Quadrotor </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../MP.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/portfolio-9.gif" class="img-fluid" alt="">
              <h5> Motion Planning of Robots in Adversarial Environments </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../pcjointkinematics.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/portfolio-6.jpg" class="img-fluid" alt="">
              <h5> Point Cloud based Joint Kinematics Prediction on MRI </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../AI.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/output.jpg" class="img-fluid" alt="">
              <h5> Enhancing sign language to text conversion by integrating lip movement recognition. </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../pblite.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/portfolio-pb.jpg" class="img-fluid" alt="" >
              <h5> PBLite: The Kernel's way </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>

          <div class="portfolio-item">
            <a href="../BE.html">
            <div class="portfolio-wrap">
              <img src="./assets/img/portfolio/FER.png" class="img-fluid" alt="">
              <h5> Facial Expression recognition </h5>
              <div class="portfolio-links">
              </div>
            </div>
          </a>
          </div>
          </div>
    
      </div>
    </section>
    <!-- ======= Contact Section ======= -->
    <section id="contact" class="contact">
      <div class="container">

        <div class="section-title">
          <h2>Contact</h2>
        </div>

        <div class="row" data-aos="fade-in">

          <div class="col-lg-5 d-flex align-items-stretch">
            <div class="info">
              <div class="address">
                <i class="bi bi-geo-alt"></i>
                <h4>Location:</h4>
                <p>37 William St, Worcester, MA 01609</p>
              </div>

              <div class="email">
                <i class="bi bi-envelope"></i>
                <h4>Email:</h4>
                <p>vmullur@wpi.edu</p>
              <!-- </div>

              <iframe src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d12097.433213460943!2d-74.0062269!3d40.7101282!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x0%3A0xb89d1fe6bc499443!2sDowntown+Conference+Center!5e0!3m2!1smk!2sbg!4v1539943755621" frameborder="0" style="border:0; width: 100%; height: 290px;" allowfullscreen></iframe>
            </div> -->

          </div>

        </div>

      </div>
    </section><!-- End Contact Section -->

  </main><!-- End #main -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="./assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="./assets/vendor/aos/aos.js"></script>
  <script src="./assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="./assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="./assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="./assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="./assets/vendor/typed.js/typed.min.js"></script>
  <script src="./assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="./assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="./assets/js/main.js"></script>

</body>

</html>
